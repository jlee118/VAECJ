{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5fd074f64a4c43b5c8c357a934cada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import pandas as pd\n",
    "\n",
    "curr_path = os.path.abspath('')\n",
    "folder_path = os.path.join(curr_path, 'data')\n",
    "filepath = os.path.join(folder_path, 'BPI_Challenge_2012.xes')\n",
    "log = pm4py.read_xes(filepath)\n",
    "df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_activities = pd.unique(pd_dataframe['concept:name'])\n",
    "activity_encoder = dict(zip(unique_activities,range(3,len(unique_activities) + 3)))\n",
    "activity_encoder['Start'] = 1\n",
    "activity_encoder['End'] = 2\n",
    "df['concept:encoded'] = df['concept:name'].apply(lambda x: activity_encoder[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_arrays(df, complete=False, W=False):\n",
    "    proc_df = df\n",
    "    if complete:\n",
    "        proc_df = proc_df[proc_df['lifecycle:transition'] == 'COMPLETE']\n",
    "    if W:\n",
    "        proc_df = proc_df[proc_df['concept:name'].str.startswith('W_')]\n",
    "    \n",
    "    arrays = proc_df.groupby(['case:concept:name']).agg(list)\n",
    "    arrays.reset_index(inplace=True)\n",
    "    arrays['time:interarrival_min'] = arrays['time:timestamp'].apply(lambda x: [0] + [0] + [((x[i+1] - x[i]).total_seconds() / 60) for i in range(len(x)-1)] + [0])\n",
    "    arrays['concept:encoded'] = arrays['concept:encoded'].apply(lambda x: [1] + x + [2])\n",
    "    return arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = process_arrays(df, complete=True, W=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "# sampled_arrays = arrays['case:concept:name'].sample(n= 100000)\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "splits = []\n",
    "for train_index, test_index in kf.split(arrays['case:concept:name']):\n",
    "    id_tr = arrays['case:concept:name'].iloc[train_index]\n",
    "    id_te = arrays['case:concept:name'].iloc[test_index]\n",
    "    splits.append((id_tr, id_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "id_train, id_test = splits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "# Many to one + context\n",
    "\n",
    "\n",
    "activity_decoder = {v:k for k,v in activity_encoder.items()}\n",
    "\n",
    "def mto_lstm_prep(journey):\n",
    "    inp = [journey[:i] for i in range(1,len(journey))]\n",
    "    out = journey[1:]\n",
    "    return (inp,out)\n",
    "\n",
    "def make_data(id_indexes, arrays_df):\n",
    "    X_j = []\n",
    "    Y_j = []\n",
    "    X_t = []\n",
    "    Y_t = []\n",
    "\n",
    "    selected = arrays_df[arrays_df[\"case:concept:name\"].isin(id_indexes)]\n",
    "\n",
    "    for index, row in selected.iterrows():\n",
    "        j_inp, j_out = mto_lstm_prep(row['concept:encoded'])\n",
    "        t_inp, t_out = mto_lstm_prep(row['time:interarrival_min'])\n",
    "        X_j.extend(j_inp)\n",
    "        X_t.extend(t_inp)\n",
    "        Y_j.extend(j_out)\n",
    "        Y_t.extend(t_out)\n",
    "    X_j = keras.preprocessing.sequence.pad_sequences(X_j, padding='pre', maxlen=60)\n",
    "    X_j = to_categorical(X_j)\n",
    "    X_t = keras.preprocessing.sequence.pad_sequences(X_t, padding='pre', maxlen=60)\n",
    "    Y_j = np.asarray(Y_j).astype(\"float32\")\n",
    "    Y_j = to_categorical(Y_j)\n",
    "    Y_t = np.asarray(Y_t).astype(\"float32\")\n",
    "    return (X_j, X_t, Y_j, Y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j_train, X_t_train, Y_j_train, Y_t_train = make_data(id_train.values, arrays)\n",
    "X_j_test, X_t_test, Y_j_test, Y_t_test = make_data(id_test.values, arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54873, 60)"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54873, 60, 27)"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_j_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Next Activity Prediction VRNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_99\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_127 (InputLayer)          [(None, 60, 27)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_128 (InputLayer)          [(None, 60, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_63 (LSTM)                  (None, 128)          79872       input_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_64 (LSTM)                  (None, 32)           4352        input_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 160)          0           lstm_63[0][0]                    \n",
      "                                                                 lstm_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_254 (Dense)               (None, 2)            322         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_255 (Dense)               (None, 2)            322         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sample_45 (Sample)              (None, 2)            0           dense_254[0][0]                  \n",
      "                                                                 dense_255[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_252 (Dense)               (None, 2)            322         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_253 (Dense)               (None, 2)            322         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_251 (Dense)               (None, 2)            66          lstm_64[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 85,578\n",
      "Trainable params: 85,578\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "lstm_dim = 128\n",
    "timesteps = X_j_train.shape[1]\n",
    "timefeat_dim = X_j_train.shape[2]\n",
    "z_dim = 2\n",
    "\n",
    "class Sample(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "input_j = layers.Input(shape=(timesteps,timefeat_dim))\n",
    "lstm = layers.LSTM(lstm_dim)(input_j)\n",
    "\n",
    "\n",
    "input_t = layers.Input(shape=(timesteps, 1))\n",
    "t_lstm = layers.LSTM(32)(input_t)\n",
    "t_output = layers.Dense(2, activation='relu')(t_lstm)\n",
    "\n",
    "merged = layers.Concatenate()([lstm, t_lstm])\n",
    "\n",
    "z_p_means = layers.Dense(z_dim)(merged)\n",
    "z_p_log_var = layers.Dense(z_dim)(merged)\n",
    "\n",
    "z_q_means = layers.Dense(z_dim)(merged)\n",
    "z_q_log_var = layers.Dense(z_dim)(merged)\n",
    "\n",
    "z = Sample()([z_q_means, z_q_log_var])\n",
    "\n",
    "encoder = keras.Model([input_j, input_t], [z, z_q_means, z_q_log_var, z_p_means, z_p_log_var, t_output])\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_129 (InputLayer)       [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_256 (Dense)            (None, 27)                81        \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 27)                0         \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_z = layers.Input(shape=(z_dim,))\n",
    "output = layers.Dense(timefeat_dim, activation='softmax')(input_z)\n",
    "output = layers.Dropout(0.5)(output)\n",
    "\n",
    "decoder = keras.Model(input_z, output)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_130 (InputLayer)       [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_257 (Dense)            (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_z = layers.Input(shape=(z_dim,))\n",
    "output = layers.Dense(1, activation=\"softplus\")(input_z)\n",
    "output = layers.Dropout(0.5)(output)\n",
    "\n",
    "t_decoder = keras.Model(input_z, output)\n",
    "t_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla VRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def custom_KL(posterior_means, prior_means, posterior_log_var, prior_log_var):\n",
    "    # KL of p under q    \n",
    "    kl = prior_log_var - posterior_log_var + (tf.exp(posterior_log_var) + \n",
    "                                       tf.square(posterior_means - prior_means)) / tf.exp(prior_log_var) - 1\n",
    "    kl = 0.5 * tf.reduce_sum(kl, axis=1)\n",
    "    return kl\n",
    "\n",
    "# total number of epochs\n",
    "n_epochs = 20\n",
    "# The number of epochs at which KL loss should be included\n",
    "klstart = 1\n",
    "# number of epochs over which KL scaling is increased from 0 to 1\n",
    "kl_annealtime = 4\n",
    "\n",
    "class AnnealingCallback(Callback):\n",
    "    def __init__(self, weight):\n",
    "        self.weight = weight\n",
    "    def on_epoch_end (self, epoch, logs={}):\n",
    "        if epoch > klstart :\n",
    "            new_weight = min(K.get_value(self.weight) + (1./ kl_annealtime), 1.)\n",
    "            K.set_value(self.weight, new_weight)\n",
    "        print (\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
    "\n",
    "class RVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, t_decoder, **kwargs):\n",
    "        super(RVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.t_decoder = t_decoder\n",
    "    def compile(self, optimizer, beta):\n",
    "        super(RVAE, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.beta = beta\n",
    "        \n",
    "    def train_step(self,data):\n",
    "        if isinstance(data, tuple):\n",
    "            journeys, times = data[0]\n",
    "            output, t_output = data[1]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            z, pos_means, pos_log_var, prior_means, prior_log_var, t_output= self.encoder([journeys, times])\n",
    "            pred_activity = self.decoder(z)\n",
    "            pred_time = self.t_decoder(z)\n",
    "            \n",
    "            journey_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(tf.keras.losses.categorical_crossentropy(output, pred_activity))\n",
    "            )\n",
    "            \n",
    "            time_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(tf.keras.losses.mean_absolute_error(t_output, pred_time))\n",
    "            )\n",
    "                        \n",
    "            t_kl_divergence =   tf.reduce_mean(\n",
    "                custom_KL(pos_means, prior_means, pos_log_var, prior_log_var)\n",
    "            )\n",
    "            \n",
    "\n",
    "\n",
    "            total_loss = journey_loss + time_loss + self.beta * (t_kl_divergence)\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"journey_loss\":journey_loss,\n",
    "            \"time_loss\":time_loss,\n",
    "            \"t_kl_divergence\": t_kl_divergence\n",
    "        }\n",
    "     \n",
    "    def test_step(self, data):\n",
    "        journeys, times = data[0]\n",
    "        outputs, t_outputs = data[1]\n",
    "        pred_activity, pred_time = self([journeys, times], training=False)\n",
    "        journey_loss = tf.keras.losses.categorical_crossentropy(outputs, pred_activity)\n",
    "        time_loss = tf.keras.losses.mean_absolute_error(t_outputs, pred_time)\n",
    "        return {\n",
    "            \"journey_loss\": journey_loss,\n",
    "            \"time_loss\":time_loss,\n",
    "        }\n",
    "            \n",
    "    def call(self, data):\n",
    "        journeys, times = data\n",
    "        z, pos_means, pos_log_var, prior_means, prior_log_var, t_output = self.encoder([journeys, times])\n",
    "        return(self.decoder(z), self.t_decoder(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1287/1287 [==============================] - 41s 29ms/step - loss: 47.4654 - journey_loss: 47.1934 - time_loss: 0.2720 - t_kl_divergence: 397.6841 - val_journey_loss: 1.4641 - val_time_loss: 1811.0100\n",
      "Current KL Weight is 0.0\n",
      "Epoch 2/20\n",
      "1287/1287 [==============================] - 38s 29ms/step - loss: 31.6864 - journey_loss: 31.6570 - time_loss: 0.0294 - t_kl_divergence: 616.6181 - val_journey_loss: 1.1888 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 0.0\n",
      "Epoch 3/20\n",
      "1287/1287 [==============================] - 37s 29ms/step - loss: 28.2150 - journey_loss: 28.1921 - time_loss: 0.0230 - t_kl_divergence: 571.0161 - val_journey_loss: 1.0200 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 0.25\n",
      "Epoch 4/20\n",
      "1287/1287 [==============================] - 41s 32ms/step - loss: 27.8930 - journey_loss: 26.8241 - time_loss: 0.0242 - t_kl_divergence: 4.1791 - val_journey_loss: 1.0378 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 0.5\n",
      "Epoch 5/20\n",
      "1287/1287 [==============================] - 38s 30ms/step - loss: 25.9147 - journey_loss: 25.6479 - time_loss: 0.0204 - t_kl_divergence: 0.4926 - val_journey_loss: 0.9941 - val_time_loss: 1811.0100\n",
      "Current KL Weight is 0.75\n",
      "Epoch 6/20\n",
      "1287/1287 [==============================] - 36s 28ms/step - loss: 25.1280 - journey_loss: 25.0628 - time_loss: 0.0186 - t_kl_divergence: 0.0622 - val_journey_loss: 0.9885 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 1.0\n",
      "Epoch 7/20\n",
      "1287/1287 [==============================] - 36s 28ms/step - loss: 24.8084 - journey_loss: 24.7459 - time_loss: 0.0163 - t_kl_divergence: 0.0463 - val_journey_loss: 1.0568 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 1.0\n",
      "Epoch 8/20\n",
      "1287/1287 [==============================] - 36s 28ms/step - loss: 24.6171 - journey_loss: 24.5586 - time_loss: 0.0142 - t_kl_divergence: 0.0443 - val_journey_loss: 1.0690 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 1.0\n",
      "Epoch 9/20\n",
      "1287/1287 [==============================] - 36s 28ms/step - loss: 24.4241 - journey_loss: 24.3759 - time_loss: 0.0134 - t_kl_divergence: 0.0348 - val_journey_loss: 0.9722 - val_time_loss: 1811.0100\n",
      "Current KL Weight is 1.0\n",
      "Epoch 10/20\n",
      "1287/1287 [==============================] - 36s 28ms/step - loss: 24.3332 - journey_loss: 24.2834 - time_loss: 0.0127 - t_kl_divergence: 0.0372 - val_journey_loss: 0.9674 - val_time_loss: 1811.0099\n",
      "Current KL Weight is 1.0\n",
      "Epoch 11/20\n",
      "1287/1287 [==============================] - 37s 28ms/step - loss: 24.2023 - journey_loss: 24.1568 - time_loss: 0.0120 - t_kl_divergence: 0.0335 - val_journey_loss: 0.9358 - val_time_loss: 1811.0096\n",
      "Current KL Weight is 1.0\n",
      "Epoch 12/20\n",
      "1287/1287 [==============================] - 36s 28ms/step - loss: 24.1552 - journey_loss: 24.1104 - time_loss: 0.0113 - t_kl_divergence: 0.0334 - val_journey_loss: 1.0370 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 1.0\n",
      "Epoch 13/20\n",
      "1287/1287 [==============================] - 36s 28ms/step - loss: 24.0880 - journey_loss: 24.0436 - time_loss: 0.0106 - t_kl_divergence: 0.0338 - val_journey_loss: 1.0008 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 1.0\n",
      "Epoch 14/20\n",
      "1287/1287 [==============================] - 37s 29ms/step - loss: 24.0081 - journey_loss: 23.9666 - time_loss: 0.0093 - t_kl_divergence: 0.0322 - val_journey_loss: 0.9765 - val_time_loss: 1811.0100\n",
      "Current KL Weight is 1.0\n",
      "Epoch 15/20\n",
      "1287/1287 [==============================] - 37s 29ms/step - loss: 23.9674 - journey_loss: 23.9303 - time_loss: 0.0085 - t_kl_divergence: 0.0286 - val_journey_loss: 0.9955 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 1.0\n",
      "Epoch 16/20\n",
      "1287/1287 [==============================] - 37s 29ms/step - loss: 23.9159 - journey_loss: 23.8725 - time_loss: 0.0158 - t_kl_divergence: 0.0276 - val_journey_loss: 1.0425 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 1.0\n",
      "Epoch 17/20\n",
      "1287/1287 [==============================] - 39s 30ms/step - loss: 23.8809 - journey_loss: 23.8452 - time_loss: 0.0076 - t_kl_divergence: 0.0281 - val_journey_loss: 0.9731 - val_time_loss: 1811.0100\n",
      "Current KL Weight is 1.0\n",
      "Epoch 18/20\n",
      "1287/1287 [==============================] - 40s 31ms/step - loss: 23.8454 - journey_loss: 23.8131 - time_loss: 0.0055 - t_kl_divergence: 0.0268 - val_journey_loss: 0.9899 - val_time_loss: 1811.0100\n",
      "Current KL Weight is 1.0\n",
      "Epoch 19/20\n",
      "1287/1287 [==============================] - 39s 30ms/step - loss: 23.8049 - journey_loss: 23.7735 - time_loss: 0.0044 - t_kl_divergence: 0.0269 - val_journey_loss: 0.9435 - val_time_loss: 1811.0101\n",
      "Current KL Weight is 1.0\n",
      "Epoch 20/20\n",
      "1287/1287 [==============================] - 38s 29ms/step - loss: 23.7888 - journey_loss: 23.7616 - time_loss: 0.0031 - t_kl_divergence: 0.0241 - val_journey_loss: 0.9695 - val_time_loss: 1811.0100\n",
      "Current KL Weight is 1.0\n"
     ]
    }
   ],
   "source": [
    "weight = K.variable(0.)\n",
    "weight._trainable = False\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "rvae = RVAE(encoder, decoder, t_decoder)\n",
    "rvae.compile(optimizer=keras.optimizers.Adam(lr=0.001), beta=weight)\n",
    "history = rvae.fit([X_j_train, X_t_train], [Y_j_train, Y_t_train], validation_split=0.25, epochs=n_epochs, \n",
    "                   batch_size=32, callbacks=[AnnealingCallback(weight)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.6430657475275449\n",
      "accuracy: 0.6597543937054196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "preds, pred_times= rvae.predict([X_j_test, X_t_test])\n",
    "pred_activities = np.argmax(preds, axis=1)\n",
    "truth_activities = np.argmax(Y_j_test, axis=1)\n",
    "print(\"f1:\", f1_score(truth_activities, pred_activities, average='weighted'))\n",
    "print(\"accuracy:\", accuracy_score(truth_activities, pred_activities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden dimension 2\n",
    "# BPI 12\n",
    "results1 = [85.11, 85.47, 84.96]\n",
    "\n",
    "# BPI 12 Complete\n",
    "\n",
    "results2 = [78.33, 78.77, 76.8]\n",
    "\n",
    "# BPI12 W\n",
    "results3  = [83.96, 84.55, 82.99]\n",
    "\n",
    "# BPI12 W Complete\n",
    "results4  = [66.09, 66.8, 64.78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['BPI 12', 'BPI 12 COMPLETE', 'BPI 12 W', 'BPI 12 W COMPLETE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BPI 12</th>\n",
       "      <th>BPI 12 COMPLETE</th>\n",
       "      <th>BPI 12 W</th>\n",
       "      <th>BPI 12 W COMPLETE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.11</td>\n",
       "      <td>78.33</td>\n",
       "      <td>83.96</td>\n",
       "      <td>66.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.47</td>\n",
       "      <td>78.77</td>\n",
       "      <td>84.55</td>\n",
       "      <td>66.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.96</td>\n",
       "      <td>76.80</td>\n",
       "      <td>82.99</td>\n",
       "      <td>64.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BPI 12  BPI 12 COMPLETE  BPI 12 W  BPI 12 W COMPLETE\n",
       "1   85.11            78.33     83.96              66.09\n",
       "2   85.47            78.77     84.55              66.80\n",
       "3   84.96            76.80     82.99              64.78"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame({'BPI 12': results1, 'BPI 12 COMPLETE':results2, 'BPI 12 W':results3, 'BPI 12 W COMPLETE':results4}, index=range(1,4))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BPI 12</th>\n",
       "      <th>BPI 12 COMPLETE</th>\n",
       "      <th>BPI 12 W</th>\n",
       "      <th>BPI 12 W COMPLETE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>85.18</td>\n",
       "      <td>77.966667</td>\n",
       "      <td>83.833333</td>\n",
       "      <td>65.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      BPI 12  BPI 12 COMPLETE   BPI 12 W  BPI 12 W COMPLETE\n",
       "mean   85.18        77.966667  83.833333              65.89"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tf.keras.losses.categorical_crossentropy(Y_j_test, preds) +\\\n",
    "tf.keras.losses.mean_absolute_error(Y_t_test, pred_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = (losses - np.min(losses)) / (np.max(losses) - np.min(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_inds = np.argwhere(score >= np.median(score) +  3 * np.std(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = np.argmax(X_j_test[outlier_inds.flatten()], axis=1)\n",
    "pred_acc = np.argmax(Y_j_test[outlier_inds.flatten()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_decoder = {v:k for k,v in activity_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "49",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-744-092f8cf22b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactivity_decoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-744-092f8cf22b49>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactivity_decoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 49"
     ]
    }
   ],
   "source": [
    "print([activity_decoder[x] for x in prefix[0] if x >0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.argmax(Y_j_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(targets == 26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred_acc == 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred_acc == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_inds = np.argwhere(pred_acc == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_outliers = np.argmax(X_j_test[end_inds.flatten(),:,:], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  1],\n",
       "       [ 0,  0,  0, ...,  6, 12, 12],\n",
       "       [ 0,  0,  0, ..., 12, 12, 12],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  6, 12, 12],\n",
       "       [ 0,  0,  0, ..., 12, 12, 12],\n",
       "       [ 0,  0,  0, ..., 12, 12, 14]])"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 23,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6])"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_outliers[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_acc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27198, 27)"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_j_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first fold 89/27198, second fold 93/28451, 88/26422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A_SUBMITTED': 3,\n",
       " 'A_PARTLYSUBMITTED': 4,\n",
       " 'A_PREACCEPTED': 5,\n",
       " 'W_Completeren aanvraag': 6,\n",
       " 'A_ACCEPTED': 7,\n",
       " 'O_SELECTED': 8,\n",
       " 'A_FINALIZED': 9,\n",
       " 'O_CREATED': 10,\n",
       " 'O_SENT': 11,\n",
       " 'W_Nabellen offertes': 12,\n",
       " 'O_SENT_BACK': 13,\n",
       " 'W_Valideren aanvraag': 14,\n",
       " 'A_REGISTERED': 15,\n",
       " 'A_APPROVED': 16,\n",
       " 'O_ACCEPTED': 17,\n",
       " 'A_ACTIVATED': 18,\n",
       " 'O_CANCELLED': 19,\n",
       " 'W_Wijzigen contractgegevens': 20,\n",
       " 'A_DECLINED': 21,\n",
       " 'A_CANCELLED': 22,\n",
       " 'W_Afhandelen leads': 23,\n",
       " 'O_DECLINED': 24,\n",
       " 'W_Nabellen incomplete dossiers': 25,\n",
       " 'W_Beoordelen fraude': 26,\n",
       " 'Start': 1,\n",
       " 'End': 2}"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research env",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
